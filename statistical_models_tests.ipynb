{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LOG): No Precompiled Dataset Found! Creating New Dataset Now...\n",
      "(LOG): Processing (0/26) => E:\\Masters\\IN5000 - Final Project\\AI-ForestWatch-Data\\Netherlands data\\train\\landsat8_28992_2015_Drenthe.tif\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-58faa893fdec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;31m#     band_combinations = [\"rgb\", \"full-spectrum\", \"augmented\", \"extended\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;31m#region, class_1_weight, class_2_weight = \"Drenthe\", 1, 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mdatapoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_or_create_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;31m#     destination_folder = \"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;31m#     for this_model in classifiers:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-58faa893fdec>\u001b[0m in \u001b[0;36mload_or_create_dataset\u001b[1;34m(region)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"(LOG): Processing ({}/{}) => {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_pickle_files_in_pickled_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_data_sample_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data_sample_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthis_small_data_sample\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0msmall_image_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmall_label_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_small_data_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0mthis_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmall_image_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mrandom_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to int"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# # get all the classifiers we want to experiment with\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.linear_model import Perceptron\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# def print_to_file(this_str, file_name):\n",
    "#     print(this_str)\n",
    "#     print(this_str, file=open(os.path.join(\"E:/Forest Cover - Redo 2020/Trainings and Results/Training Data/Clipped dataset/\"\n",
    "#                                            \"statistical_models_dataset/15-Districts/\", file_name), \"a\"))\n",
    "\n",
    "\n",
    "# def train_and_test_statistical_model(name, classifier, x_train, y_train, x_test, y_test, process_name):\n",
    "#     # fit the model on your dataset\n",
    "#     trained_classifier = classifier.fit(x_train, y_train)\n",
    "#     # get predictions on unseen data\n",
    "#     y_pred = trained_classifier.predict(x_test)\n",
    "#     # get an accuracy score please\n",
    "#     accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "#     # get confusion matrix\n",
    "#     confusion_matrix_to_print = confusion_matrix(y_test, y_pred)\n",
    "#     # show confusion matrix and classification report for precision, recall, F1-score\n",
    "#     print_to_file(\"################################ {} ################################\".format(name), file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file('Model Accuracy: {:.2f}%'.format(100*accuracy), file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file('Confusion Matrix', file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file(confusion_matrix_to_print, file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file('Classification Report', file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file(classification_report(y_test, y_pred, target_names=['Non-Forest', 'Forest']), file_name=f\"{process_name}.txt\")\n",
    "#     return trained_classifier\n",
    "\n",
    "\n",
    "def load_or_create_dataset(region):\n",
    "    regions = [\"Drenthe\", \"Flevoland\", \"Limburg\", \"Gelderland\", \"Friesland\", \"Groningen\", \"Noord_Brabant\", \"Overijssel\", \"Noord_Holland\", \"Zuid_Holland\", \"Zeeland\", \"Utrecht\"]\n",
    "    years = [\"2015\",\"2017\"]\n",
    "    count = 1\n",
    "    raw_dataset_path = r\"E:\\Masters\\IN5000 - Final Project\\AI-ForestWatch-Data\\Netherlands data\\train\"\n",
    "    processed_dataset_path = r\"E:\\Masters\\IN5000 - Final Project\\AI-ForestWatch-Data\\Netherlands data\\train\\training_data\"\n",
    "    print(\"(LOG): No Precompiled Dataset Found! Creating New Dataset Now...\")\n",
    "    all_pickle_files_in_pickled_dataset = os.listdir(raw_dataset_path)\n",
    "    datapoints_as_array, labels_as_array = np.empty(shape=[1, 18]), np.empty(shape=[1, ])\n",
    "    np.random.seed(232)\n",
    "    num_samples = 1500\n",
    "    for idx, this_pickled_file in enumerate(all_pickle_files_in_pickled_dataset):\n",
    "        full_data_sample_path = os.path.join(raw_dataset_path, this_pickled_file)\n",
    "        if idx % 100 == 0:\n",
    "            print(\"(LOG): Processing ({}/{}) => {}\".format(idx, len(all_pickle_files_in_pickled_dataset), full_data_sample_path))\n",
    "        with open(full_data_sample_path, 'rb') as this_small_data_sample:\n",
    "            small_image_sample, small_label_sample = pickle.load(this_small_data_sample, encoding='latin1')\n",
    "            this_shape = small_image_sample.shape\n",
    "            random_rows, random_cols = np.random.randint(0, this_shape[0], size=num_samples), np.random.randint(0, this_shape[0], size=num_samples)\n",
    "            sample_datapoints = np.nan_to_num(small_image_sample[random_rows, random_cols, :])\n",
    "            sample_labels = np.nan_to_num(small_label_sample[random_rows, random_cols])\n",
    "            # pick only valid (not NULL) pixels\n",
    "            valid_samples = (sample_labels != 0)\n",
    "            sample_datapoints = sample_datapoints[valid_samples]\n",
    "            sample_labels = sample_labels[valid_samples]   \n",
    "            # apply the following code if you want 18 bands in your sample points\n",
    "            # get more indices to add to the example, landsat-8\n",
    "            ndvi_band = (sample_datapoints[:, 4] - sample_datapoints[:, 3]) / (sample_datapoints[:, 4] + sample_datapoints[:, 3] + 1e-3)\n",
    "            evi_band = 2.5 * (sample_datapoints[:, 4] - sample_datapoints[:, 3]) / (sample_datapoints[:, 4] + 6 * sample_datapoints[:, 3] - 7.5 * sample_datapoints[:, 1] + 1)\n",
    "            savi_band = 1.5 * (sample_datapoints[:, 4] - sample_datapoints[:, 3]) / (sample_datapoints[:, 4] + sample_datapoints[:, 3] + 0.5)\n",
    "            msavi_band = 0.5 * (2 * sample_datapoints[:, 4] + 1 - np.sqrt((2 * sample_datapoints[:, 4] + 1) ** 2 - 8 * (sample_datapoints[:, 4] - sample_datapoints[:, 3])))\n",
    "            ndmi_band = (sample_datapoints[:, 4] - sample_datapoints[:, 5]) / (sample_datapoints[:, 4] + sample_datapoints[:, 5] + 1e-3)\n",
    "            nbr_band = (sample_datapoints[:, 4] - sample_datapoints[:, 6]) / (sample_datapoints[:, 4] + sample_datapoints[:, 6] + 1e-3)\n",
    "            nbr2_band = (sample_datapoints[:, 5] - sample_datapoints[:, 6]) / (sample_datapoints[:, 5] + sample_datapoints[:, 6] + 1e-3)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(ndvi_band, axis=1)), axis=1)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(evi_band, axis=1)), axis=1)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(savi_band, axis=1)), axis=1)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(msavi_band, axis=1)), axis=1)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(ndmi_band, axis=1)), axis=1)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(nbr_band, axis=1)), axis=1)\n",
    "            sample_datapoints = np.concatenate((sample_datapoints, np.expand_dims(nbr2_band, axis=1)), axis=1)\n",
    "        datapoints_as_array = np.concatenate((datapoints_as_array, sample_datapoints), axis=0)\n",
    "        labels_as_array = np.concatenate((labels_as_array, sample_labels), axis=0)\n",
    "        # at this point, we just serialize the arrays and save them\n",
    "        with open(processed_dataset_path, 'wb') as processed_dataset:\n",
    "            pickle.dump((datapoints_as_array, labels_as_array), processed_dataset)\n",
    "        print(\"(LOG): Dataset Size: Datapoints = {}; Ground Truth Labels {}\".format(datapoints_as_array.shape, labels_as_array.shape))\n",
    "        print(\"(LOG): Compiled and Serialized New Dataset Successfully!\")\n",
    "    # fix before return\n",
    "    labels_as_array[labels_as_array == 0] = 1\n",
    "    labels_as_array -= 1  # all labels should be 0 or 1 (non-Forest, Forest)\n",
    "    return datapoints_as_array, labels_as_array\n",
    "\n",
    "\n",
    "# def train_stat_model(region, this_model, these_bands, class_1_weight, class_2_weight, datapoints_as_array, labels_as_array, process_name):\n",
    "#     model_path = f\"E:/Forest Cover - Redo 2020/Trainings and Results/Training Data/Clipped dataset/statistical_models_dataset/15-Districts/\" \\\n",
    "#                  f\"{process_name}-{these_bands}.pkl\"\n",
    "#     assert this_model == \"LogisticRegression\" or this_model == \"DecisionTreeClassifier\" or this_model == \"RandomForestClassifier\"\n",
    "#     assert these_bands == \"rgb\" or these_bands == \"full-spectrum\" or these_bands == \"augmented\" or these_bands == \"extended\"\n",
    "#     print_to_file(f\"(LOG): Parameters: {region} dataset with {this_model} and {these_bands} bands using C1W: {class_1_weight} and C2W: \"\n",
    "#                   f\"{class_2_weight}\", file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file(f\"(LOG): Will save trained model @ {model_path}\", file_name=f\"{process_name}.txt\")\n",
    "#     bands_lists = {\"rgb\": [3, 2, 1], \"full-spectrum\": [*range(11)], \"augmented\": [*range(18)], \"extended\": [*range(11, 18)]}\n",
    "#     # get your model (RandomForestClassifier, DecisionTreeClassifier, LogisticRegression)\n",
    "#     classifiers = {\n",
    "#         \"LogisticRegression\": LogisticRegression(verbose=1, n_jobs=4, max_iter=1000, solver='lbfgs', class_weight={0: class_1_weight, 1: class_2_weight}),\n",
    "#         \"DecisionTreeClassifier\": DecisionTreeClassifier(class_weight={0: class_1_weight, 1: class_2_weight}),\n",
    "#         \"RandomForestClassifier\": RandomForestClassifier(verbose=1, n_jobs=4, class_weight={0: class_1_weight, 1: class_2_weight}),\n",
    "#     }\n",
    "#     # create training and testing arrays from loaded data\n",
    "#     total_datapoints = len(datapoints_as_array)\n",
    "#     split = int(0.8*total_datapoints)\n",
    "#     # 1:4 implies RGB Model\n",
    "#     x_train, y_train = datapoints_as_array[:split, bands_lists[these_bands]], labels_as_array[:split].astype(np.uint8)\n",
    "#     x_test, y_test = datapoints_as_array[split:, bands_lists[these_bands]], labels_as_array[split:].astype(np.uint8)\n",
    "#     print_to_file(\"(LOG): Dataset for Training and Testing Prepared\", file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file(\"(LOG): Training Data: {}; Testing Data: {}\".format(x_train.shape, x_test.shape), file_name=f\"{process_name}.txt\")\n",
    "#     # call model for training\n",
    "#     trained_classifier = train_and_test_statistical_model(name=f\"{this_model}-{these_bands}\", classifier=classifiers[this_model],\n",
    "#                                                           x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, process_name=process_name)\n",
    "#     with open(model_path, 'wb') as model_file:\n",
    "#         print_to_file(pickle.dump(trained_classifier, model_file), file_name=f\"{process_name}.txt\")\n",
    "#     print_to_file(\"(LOG): Saved Trained Classifier as {}\".format(model_path), file_name=f\"{process_name}.txt\")\n",
    "#     pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # region = sys.argv[1]  # \"100K\"\n",
    "    # this_model = sys.argv[2]  # \"RandomForestClassifier\"\n",
    "    # these_bands = sys.argv[3]\n",
    "    # class_1_weight, class_2_weight = float(sys.argv[4]), float(sys.argv[5])\n",
    "#     classifiers = [\"LogisticRegression\", \"DecisionTreeClassifier\", \"RandomForestClassifier\"]\n",
    "#     band_combinations = [\"rgb\", \"full-spectrum\", \"augmented\", \"extended\"]\n",
    "    #region, class_1_weight, class_2_weight = \"Drenthe\", 1, 1\n",
    "    datapoints, labels = load_or_create_dataset(region)\n",
    "#     destination_folder = \"\"\n",
    "#     for this_model in classifiers:\n",
    "#         for these_bands in band_combinations:\n",
    "#             process_name = f\"{this_model}_{region}_C1W_{class_1_weight}_C2W_{class_2_weight}\"\n",
    "#             train_stat_model(region, this_model, these_bands, class_1_weight, class_2_weight, datapoints_as_array=datapoints, labels_as_array=labels,\n",
    "#                              process_name=process_name)\n",
    "#             pass\n",
    "#         pass\n",
    "#     pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
